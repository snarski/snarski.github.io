<!doctype html><html lang=en-us><head><title>Beating the Cramér-Rao lower bound in free energy calculations | snarski</title><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="The Cramér-Rao bound is a result in statistics that puts a lower bound on the variance of an unbiased estimator. When an unbiased estimator achieves this lower bound, there is usually not much one can do to &ldquo;improve&rdquo; the estimator. Broadly speaking, maximum likelihood estimators (MLEs) achieve this lower bound1, and the MLE in free energy calculations is sometimes called Multi-State Bennett Acceptance Ratio (MBAR)2345. It is widely accepted that the ideal setting to obtain the lowest possible variance estimate of a free energy difference is to obtain independent samples from a target mixture distribution and use MBAR to process the data."><meta name=generator content="Hugo 0.83.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/css/style.css><link rel="shortcut icon" href=/images/favicon.ico type=image/x-icon><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css integrity=sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js integrity=sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script src=https://snarski.github.io/js/mathjax-config.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js>TeX:{equationNumbers:{autoNumber:"AMS"}}</script></head><body><nav class=navigation><a href=/><span class=arrow>←</span>Home</a>
<a href=/posts>Archive</a>
<a href=/about>About</a>
<a href=/bounties>Bounties</a>
<a href=/papers>Papers</a></nav><main class=main><header class=profile><img class=avatar alt=avatar src=/images/avatar.png><h1>snarski</h1></header><section id=single><h1 class=title>Beating the Cramér-Rao lower bound in free energy calculations</h1><div class=tip><span>770 words</span>
<span class=split>·</span>
<span>4 minute read</span></div><div class=content><p>The <a href=https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound target=_blank rel=noopener>Cramér-Rao bound</a> is a result in statistics that puts a lower bound on the variance of an unbiased estimator. When an unbiased estimator achieves this lower bound, there is usually not much one can do to &ldquo;improve&rdquo; the estimator. Broadly speaking, maximum likelihood estimators (MLEs) achieve this lower bound<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, and the MLE in free energy calculations is sometimes called Multi-State Bennett Acceptance Ratio (MBAR)<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup><sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup><sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup><sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>. It is widely accepted that the ideal setting to obtain the lowest possible variance estimate of a free energy difference is to obtain independent samples from a target mixture distribution and use MBAR to process the data.</p><p>It turns out there is a better way. The <a href=https://arxiv.org/abs/2112.05109 target=_blank rel=noopener>Times Square sampling (TSS) paper</a> proves that it is possible to achieve lower variance than the MLE and beat the Cramér-Rao bound (see Section 3.4 in the Supplementary Materials). That paper is long, which makes it difficult to approach. The goal of this post is to quickly convey the main idea and raise some questions in this area.</p><p>Let $\rho_k = e^{F_k^\star - H_k(x)}$, $k = 1, &mldr;, K$, denote probability densities on $\mathbb{R}^n$, where $F_k^\star = -\log\int_{\mathbb{R}^n} e^{-H_k(x)} d\mathrm{x}$, and let $\pi = (\pi_1, \dots, \pi_K)$ denote a distribution on $[K] = \{1, \dots, K\}$. Suppose $X_1, \dots, X_N$ are independent and identically distributed samples from the mixture density $p(x) = \sum_{k=1}^K \pi_k \rho_k(x)$. The MBAR estimate $F = (F_1, \dots, F_K)$ for $F^\star = (F_1^\star, \dots, F_K^\star)$ is given by the solution to the system of equations:
\begin{equation}
e^{-F_k} = \frac{1}{N}\sum_{i=1}^N \frac{e^{-H_k(X_i)}}{\sum_{\ell=1}^K \pi_\ell e^{F_\ell - H_\ell(X_i)}},\quad k=1,\dots,K.
\end{equation}
Note that if $F = (F_1,\dots, F_K)$ is a solution to (1), then so is $(F_1+c, \dots, F_K+c)$ for any $c \in \mathbb{R},$ which simply reflects that only free energy *differences* can be estimated. For simplicity, we will always assume that when we write $F_k$, we really mean $F_k - F_0$.</p><p>A simple but important observation is that using (1) entails a two-step process:</p><ul><li>first, generate the samples $X_1, \dots, X_N$ (using, for instance, replica exchange or its infinite swapping variant);</li><li>second, provide the samples to the estimator (1) and solve the system of equations.</li></ul><p>This two-step process <em>divorces</em> the sampling from the estimation. Indeed, the estimator has no effect on the generation of the samples, and is insensitive to the sampling mechanism or the order in which the samples are provided (any permutation of $X_1, \dots, X_N$ yields the same estimate $F$). The central thrust of the TSS paper is that <em>blending</em> sampling and estimation can be of tremendous practical value.</p><p>The mechanism which enables sampling and estimation to occur concurrently is sometimes known as on-the-fly estimation, given by the recursion:
\begin{equation}
F_k^{N+1} = F_k^N + \frac{1}{N+1}\left(1 - \frac{e^{F_k^N - H_k(X_{N+1})}}{\sum_{\ell=1}^K \pi_\ell e^{F_\ell^N - H_\ell(X_{N+1})}}\right).
\end{equation}
Within the current setup, the stochastic approximation procedure with Markovian noise assumes that, for a fixed estimate $F \in \mathbb{R}^K$, a pair $(X,K)$ is generated through a Markov transition kernel that has $p(x,k;F) \propto \pi_k e^{F_k - H_k(x)}$ as its stationary density.</p><p>In other words, the next <em>sample</em> is affected by the current <em>estimate</em>. The ultimate consequence of this observation is that, if we are actually able to generate samples directly from $p(x,k;F)$ for any $F$, then the asymptotic variance of the estimate $F^N$ is<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> the $K \times K$ overlap matrix $\mathcal{O}$, given by
\begin{equation}
\mathcal{O_{ij}} = \int_{\mathbb{R}^n} \frac{\rho_i(x) \rho_j(x)}{\sum_{\ell=1}^K \pi_\ell \rho_\ell(x)} d\mathrm{x}.
\end{equation}
On the other hand, the asymptotic variance of the MBAR estimator is
\begin{equation}
(\Pi - \Pi \mathcal{O} \Pi)^+ - \Pi^{-1},
\end{equation}
where $\Pi = \mathrm{diag}(\pi)$ is the $K\times K$ diagonal matrix with $(k,k)$ entry equal to $\pi_k$, and the superscript $+$ denotes a Moore-Penrose pseudo-inverse. The key result mentioned at the beginning of this post follows after we make rigorous the following heuristic calculation (the inequalities are in the Loewner ordering on positive semi-definite matrices):
\begin{align}
&(\Pi - \Pi \mathcal{O} \Pi)^+ - \Pi^{-1} \geqslant \mathcal{O}\\<br>\iff \quad& I - (I - \Pi \mathcal{O} ) \geqslant \Pi \mathcal{O} - \Pi \mathcal{O} \Pi \mathcal{O} \\<br>\iff \quad& (\Pi \mathcal{O})^2 \geqslant 0.
\end{align}</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Theorem 7.6.3 in M. Schervish&rsquo;s <em>Theory of Statistics</em>, but it is an easily Google-able result.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>The <a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2671659/ target=_blank rel=noopener>official paper</a> or a <a href=https://arxiv.org/pdf/1704.00891.pdf target=_blank rel=noopener>post-facto realization</a> of how most people had derived MBAR in the past.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>A somewhat <a href=http://www.stat.rutgers.edu/home/ztan/Publication/Kong-et-al-2003.pdf target=_blank rel=noopener>controversial paper</a> that goes into a bit more detail about MBAR. The best reference is Vardi&rsquo;s paper from 1985, but uses less modern language.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>Estimating normalizing constants and reweighting mixtures in markov chain Monte Carlo. Technical Report No. 568, School of Statistics, University of Minnesota, 1994.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p>Empirical Distributions in Selection Bias Models, 1985.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6 role=doc-endnote><p>See Section 3 in the Supplementary Materials of the TSS paper.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div></section></main><footer id=footer><div class=copyright>© Copyright
2022
<span class=split><svg fill="#bbb" width="15" height="15" id="heart-15" xmlns="http://www.w3.org/2000/svg" width="15" height="15" viewBox="0 0 15 15"><path d="M13.91 6.75c-1.17 2.25-4.3 5.31-6.07 6.94-.1903.1718-.4797.1718-.67.0C5.39 12.06 2.26 9 1.09 6.75-1.48 1.8 5-1.5 7.5 3.45 10-1.5 16.48 1.8 13.91 6.75z"/></svg></span></div><div class=powerby>Powered by <a href=http://www.gohugo.io/>Hugo</a> Theme By <a href=https://github.com/nodejh/hugo-theme-cactus-plus>nodejh</a></div></footer></body></html>